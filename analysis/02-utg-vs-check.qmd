---
title: "UTG PRF vs. BB: BB checks flop"
format: html
execute: 
    echo: false
code-fold: true
---

```{r}
#| echo: true
library(tidyverse)
library(here)
workbook <- here("data-raw", "163-srp-utg-vs-bb-flat.gto.xlsx")

# bb <- readxl::read_xlsx(workbook, sheet = "BB") |> 
#    filter(!is.na(Tree),
#        Tree != "163 flops")

utg_v_check <- readxl::read_xlsx(workbook, sheet = "UTG v Check") |> 
    filter(!is.na(Tree),
        Tree != "163 flops")

# utg_v_bet4.5 <- readxl::read_xlsx(workbook, sheet = "UTG v Bet4.5") |> 
#    filter(!is.na(Tree),
#        Tree != "163 flops")

ranges <- readxl::read_xlsx(workbook, sheet = "Ranges")
```

The first notebook looked briefly at the actions of a BB caller to an UTG preflop raiser, where the BB player is first to act on the flop. But the BB player hardly ever will lead with a bet in this situation, regardless of the flop.[^1]

[^1]: Although it is interesting to know when these situations exist.

In this workbook we look to see how UTG responds to the almost-certain check by the BB player.

In the last notebook, we mentioned the difficulty that comes with using the probabilities from each of the actions as variables to perform clustering over an array of variables that sum to one. A change in one of these variables necessarily produces a change in the other variables. One way to diminish the effect of that is to take the square root of the probabilities. With only two betting options (checking, and betting 4.5 big blinds), we were easily able to transform the d variables into a d-1 space, using a reference variable. 

But here our UTG player has three betting options: check; bet 3 big blinds, and bet 5.9 big blinds. more generalizable approach would be needed if we begin to give the solver 3 or more options at each node. We attempt one below.

```{r}
source(here("R", "compositional.R"))

action_cols <- c("Bet 5.9", "Bet 3", "Check")

utg_v_check <- utg_v_check |>
    add_compositional_distance(action_cols = action_cols)
```

Mahalanobis distance measures how far a point is from the center of a distribution, accounting for the correlation structure of the variables. It is expressed in units of standard deviations.

Low values (close to zero) indicate the observation is similar to the typical observation in the distribution, indicating good balance or similarity between groups. 

High value sindicate the observation is far from the typical observation, indicating poor balance or dissimilarity between groups. 

In other words, the Mahalanobis measure captures how unusual a flop's strategy is, compared to the other flops. **This might suggest these are worth study as outliers or strategic exemplars.** 

We can now start clustering these flops together using K-means. We'll try a few different options of K to see what makes sense, but start with K = 3 again. 

```{r}
# K-means model spec, workflow, recipe, and fit below
# Cluster using K-means with K=3, using the variables in cluster_variables
cluster_variables <- c("Equity", "EV", "Bet 5.9_ref", "Bet 3_ref")

library(tidymodels)
library(tidyclust) # provides k_means() and extract_cluster_assignment()
set.seed(67)

# K-means model specification
kmeans_spec_3 <- k_means(num_clusters = 3)

# Recipe: normalize all predictors
# Not sure if I should be normalizing the `Bet 5.9_ref` and `Bet 3_ref` columns...
kmeans_rec <- recipe(~., data = utg_v_check |> select(all_of(cluster_variables))) |>
    step_normalize(all_numeric_predictors())

# Workflow
kmeans_wflow_3 <- workflow() |>
    add_recipe(kmeans_rec) |>
    add_model(kmeans_spec_3)

# Fit
kmeans_fit_3 <- kmeans_wflow_3 |>
    fit(data = utg_v_check |> select(all_of(cluster_variables)))

# Extract cluster assignments
flops_clustered <- utg_v_check |>
    mutate(cluster_3 = extract_cluster_assignment(kmeans_fit_3)$.cluster)
```

```{r}
# View cluster summary
flops_clustered |>
    count(cluster_3)
```

Now let's plot the clusters in a 3D space of Equity, EV, and Check.[^2] The clustering was done in four dimensions and this plot is in three dimensions, so the clusters won't appear as sensible. 

[^2]: While `Check` itself was not directly included as a clustering variable, it is implicitly captured through the reference transformation. The `Bet 5.9_ref` and `Bet 3_ref` variables used in clustering are ratios of the form $\sqrt{\text{Bet}} / \sqrt{\text{Check}}$, meaning Check serves as the denominator in both transformed variables. Since action probabilities form a simplex (summing to 1), the Check probability is inversely related to the betting probabilities. 

```{r}
library(plotly)
```

```{r}
flops_clustered |> 
    mutate(cluster_3 = factor(cluster_3)) |> 
    plot_ly(
        x = ~Equity,
        y = ~EV,
        z = ~Check,
        color = ~cluster_3,
        text = ~Tree,
        hovertemplate = paste(
            "Flop: %{text}<br>",
            "Equity: %{x:.3f}<br>",
            "EV: %{y:.3f}<br>",
            "Check: %{z:.3f}<br>"
        ),
        colors = c("#1f77b4", "#ff7f0e", "#2ca02c"),
        type = "scatter3d",
        mode = "markers",
        marker = list(size = 4, opacity = 0.8)
    ) |> 
    layout(
        scene = list(
            xaxis = list(title = "Equity"),
            yaxis = list(title = "EV"),
            zaxis = list(title = "Check")
        ),
        legend = list(title = list(text = "Cluster (k=3)"))
    )
```

Despite the caveats about clustering in four dimensions and visualizing in only three, this doesn't look very informative. It's also noteworthy that the third cluster (in orange) only has 11 flops in it. Perhaps more than 3 clusters may be in order. We try 5 clusters in the code below:

```{r}
# K-means model specification
kmeans_spec_5 <- k_means(num_clusters = 5)

# Recipe: normalize all predictors
# Not sure if I should be normalizing the `Bet 5.9_ref` and `Bet 3_ref` columns...
kmeans_rec <- recipe(~., data = utg_v_check |> select(all_of(cluster_variables))) |>
    step_normalize(all_numeric_predictors())

# Workflow
kmeans_wflow_5 <- workflow() |>
    add_recipe(kmeans_rec) |>
    add_model(kmeans_spec_5)

# Fit
kmeans_fit_5 <- kmeans_wflow_5 |>
    fit(data = utg_v_check |> select(all_of(cluster_variables)))

# Extract cluster assignments
flops_clustered <- utg_v_check |>
    mutate(cluster_5 = extract_cluster_assignment(kmeans_fit_5)$.cluster)

# View cluster summary
flops_clustered |>
    count(cluster_5)
```

It's not looking so good -- we have one cluster with only 3 flops in it.

```{r}
flops_clustered |> 
    mutate(cluster_5 = factor(cluster_5)) |> 
    plot_ly(
        x = ~Equity,
        y = ~EV,
        z = ~Check,
        color = ~cluster_5,
        text = ~Tree,
        hovertemplate = paste(
            "Flop: %{text}<br>",
            "Equity: %{x:.3f}<br>",
            "EV: %{y:.3f}<br>",
            "Check: %{z:.3f}<br>"
        ),
        colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#dae50dff", "#680101ff"),
        type = "scatter3d",
        mode = "markers",
        marker = list(size = 4, opacity = 0.8)
    ) |> 
    layout(
        scene = list(
            xaxis = list(title = "Equity"),
            yaxis = list(title = "EV"),
            zaxis = list(title = "Check")
        ),
        legend = list(title = list(text = "Cluster (k=5)"))
    )
```

We can try K=4, which might be best for clustering with these variables, but perhaps we need to consider some other way of performing clustering, or some other variables to cluster on. It's tempting to transform the bet frequency columns into weighted bet amounts, but that might obscure instances where we have a polarized betting strategy. Perhaps we can try K-means where K=3 and includes the Mahalanobis measure as well as EV and Equity.

Just for laughs let's cluster with K = 4 using the same variables we have been using.

```{r}
# K-means model specification
kmeans_spec_4 <- k_means(num_clusters = 4)

# Recipe: normalize all predictors
# Not sure if I should be normalizing the `Bet 5.9_ref` and `Bet 3_ref` columns...
kmeans_rec <- recipe(~., data = utg_v_check |> select(all_of(cluster_variables))) |>
    step_normalize(all_numeric_predictors())

# Workflow
kmeans_wflow_4 <- workflow() |>
    add_recipe(kmeans_rec) |>
    add_model(kmeans_spec_4)

# Fit
kmeans_fit_4 <- kmeans_wflow_4 |>
    fit(data = utg_v_check |> select(all_of(cluster_variables)))

# Extract cluster assignments
flops_clustered <- utg_v_check |>
    mutate(cluster_4 = extract_cluster_assignment(kmeans_fit_4)$.cluster)

# View cluster summary
flops_clustered |>
    count(cluster_4)
```

It again has a cluster with very few units in it (interestingly, the same flops as in K=5!).

And then let's plot it in a 3D space with Check as the z-axis:

```{r}
flops_clustered |> 
    mutate(cluster_4 = factor(cluster_4)) |> 
    plot_ly(
        x = ~Equity,
        y = ~EV,
        z = ~Check,
        color = ~cluster_4,
        text = ~Tree,
        hovertemplate = paste(
            "Flop: %{text}<br>",
            "Equity: %{x:.3f}<br>",
            "EV: %{y:.3f}<br>",
            "Check: %{z:.3f}<br>"
        ),
        colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#dae50dff"),
        type = "scatter3d",
        mode = "markers",
        marker = list(size = 4, opacity = 0.8)
    ) |> 
    layout(
        scene = list(
            xaxis = list(title = "Equity"),
            yaxis = list(title = "EV"),
            zaxis = list(title = "Check")
        ),
        legend = list(title = list(text = "Cluster (k=4)"))
    )
```

Some quick summary statistics:

```{r}
flops_clustered |>
    group_by(cluster_4) |>
    summarize(n = n(),
        Equity = mean(Equity),
        EV = mean(EV),
        EQR = mean(EV/(Equity/100)),
        bet_5.9 = mean(`Bet 5.9`),
        bet_3 = mean(`Bet 3`),
        check = mean(Check),
        mahalanobis = mean(log1p(mahalanobis)))
```

Cluster 3 with K=4 is hard to visually distinguish from Cluster 2, and seemingly only exists because we are forcing the algorithm to find 4 clusters.

```{r}
# Perhaps an elbow plot for K=3, K=4, K=5?
```

And now we can get on with using the Mahalanobis measure (or its square root or a log transformation) in a K=3 K-means clustering.

```{r}
# K-means clustering with Mahalanobis
cluster_variables_maha <- c("Equity", "EV", "mahalanobis")

# K-means model specification
kmeans_spec_3_maha <- k_means(num_clusters = 3)

# Recipe: log1p transform mahalanobis, then normalize all predictors
kmeans_rec_maha <- recipe(~., data = utg_v_check |> select(all_of(cluster_variables_maha))) |>
    step_mutate(mahalanobis = log1p(mahalanobis)) |>
    step_normalize(all_numeric_predictors())

# Workflow
kmeans_wflow_3_maha <- workflow() |>
    add_recipe(kmeans_rec_maha) |>
    add_model(kmeans_spec_3_maha)

# Fit
kmeans_fit_3_maha <- kmeans_wflow_3_maha |>
    fit(data = utg_v_check |> select(all_of(cluster_variables_maha)))

# Extract cluster assignments
flops_clustered <- utg_v_check |>
    mutate(cluster_3_maha = extract_cluster_assignment(kmeans_fit_3_maha)$.cluster)

# View cluster summary
flops_clustered |>
    count(cluster_3_maha)
```

There are a reasonable number of flops in each cluster. That seems good. Some summary statistics:

```{r}
flops_clustered |>
    group_by(cluster_3_maha) |>
    summarize(n = n(),
        Equity = mean(Equity),
        EV = mean(EV),
        EQR = mean(EV/(Equity/100)),
        bet_5.9 = mean(`Bet 5.9`),
        bet_3 = mean(`Bet 3`),
        check = mean(Check),
        mahalanobis = mean(log1p(mahalanobis)))
```

Cluster 2 has a relatively high Mahalanobis measure and a very low checking frequency. It also has the highest Equity, highest EV, and the highest Equity Realization Rate (EQR). Might it be said that these are the flops where UTG makes their money?

And visualizing it in three dimensions.

```{r}
flops_clustered |> 
    mutate(cluster_3_maha = factor(cluster_3_maha)) |> 
    plot_ly(
        x = ~Equity,
        y = ~EV,
        z = ~log1p(mahalanobis),
        color = ~cluster_3_maha,
        text = ~Tree,
        hovertemplate = paste(
            "Flop: %{text}<br>",
            "Equity: %{x:.3f}<br>",
            "EV: %{y:.3f}<br>",
            "Mahalanobis: %{z:.3f}<br>"
        ),
        colors = c("#1f77b4", "#ff7f0e", "#2ca02c"),
        type = "scatter3d",
        mode = "markers",
        marker = list(size = 4, opacity = 0.8)
    ) |> 
    layout(
        scene = list(
            xaxis = list(title = "Equity"),
            yaxis = list(title = "EV"),
            zaxis = list(title = "Mahalanobis Distance")
        ),
        legend = list(title = list(text = "Cluster (k=3, Maha)"))
    )
```

# Regressions
Out of curiosity, I wanted to regress EV on the other variables. (Add code blocks here)